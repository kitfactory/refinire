from __future__ import annotations

"""GenAgent â€” Modern LLMPipeline-based Step for Flow workflows.

GenAgentã¯LLMPipelineã‚’Stepã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ€ãƒ³ãªã‚¯ãƒ©ã‚¹ã§ã™ã€‚
ç”Ÿæˆã€è©•ä¾¡ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’Flow/Stepã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å†…ã§æä¾›ã—ã¾ã™ã€‚
"""

import warnings
from typing import Any, Callable, List, Dict, Optional, Type

from ..step import Step
from ..context import Context
from .llm_pipeline import LLMPipeline, LLMResult


# GenAgent implementation using LLMPipeline
# LLMPipelineã‚’ä½¿ç”¨ã™ã‚‹GenAgentå®Ÿè£…

class GenAgent(Step):
    """
    Modern Step implementation using LLMPipeline instead of deprecated AgentPipeline
    éæ¨å¥¨ã®AgentPipelineã«ä»£ã‚ã£ã¦LLMPipelineã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ€ãƒ³ãªStepå®Ÿè£…
    
    This class provides generation, evaluation, and retry capabilities within Flow workflows
    without depending on the deprecated AgentPipeline.
    ã“ã®ã‚¯ãƒ©ã‚¹ã¯éæ¨å¥¨ã®AgentPipelineã«ä¾å­˜ã™ã‚‹ã“ã¨ãªãã€Flowãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å†…ã§
    ç”Ÿæˆã€è©•ä¾¡ã€ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
    """

    def __init__(
        self,
        name: str,
        generation_instructions: str,
        evaluation_instructions: Optional[str] = None,
        *,
        output_model: Optional[Type[Any]] = None,
        model: str = "gpt-4o-mini",
        evaluation_model: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        timeout: float = 30.0,
        threshold: float = 85.0,
        max_retries: int = 3,
        input_guardrails: Optional[List[Callable[[str], bool]]] = None,
        output_guardrails: Optional[List[Callable[[Any], bool]]] = None,
        session_history: Optional[List[str]] = None,
        history_size: int = 10,
        improvement_callback: Optional[Callable[[LLMResult, Any], str]] = None,
        locale: str = "en",
        next_step: Optional[str] = None,
        store_result_key: Optional[str] = None,
        tools: Optional[List[Dict]] = None,
        mcp_servers: Optional[List[str]] = None,
    ) -> None:
        """
        Initialize GenAgent with LLMPipeline configuration
        LLMPipelineè¨­å®šã§GenAgentã‚’åˆæœŸåŒ–ã™ã‚‹

        Args:
            name: Step name / ã‚¹ãƒ†ãƒƒãƒ—å
            generation_instructions: System prompt for generation / ç”Ÿæˆç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            evaluation_instructions: System prompt for evaluation / è©•ä¾¡ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            output_model: Pydantic model for structured output / æ§‹é€ åŒ–å‡ºåŠ›ç”¨Pydanticãƒ¢ãƒ‡ãƒ«
            model: LLM model name / LLMãƒ¢ãƒ‡ãƒ«å
            evaluation_model: Optional LLM model name for evaluation / è©•ä¾¡ç”¨LLMãƒ¢ãƒ‡ãƒ«åï¼ˆä»»æ„ï¼‰
            temperature: Sampling temperature / ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦
            max_tokens: Maximum tokens / æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            timeout: Request timeout / ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            threshold: Evaluation score threshold / è©•ä¾¡ã‚¹ã‚³ã‚¢é–¾å€¤
            max_retries: Number of retry attempts / ãƒªãƒˆãƒ©ã‚¤è©¦è¡Œå›æ•°
            input_guardrails: Input validation functions / å…¥åŠ›æ¤œè¨¼é–¢æ•°
            output_guardrails: Output validation functions / å‡ºåŠ›æ¤œè¨¼é–¢æ•°
            session_history: Session history / ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´
            history_size: Size of history to keep / ä¿æŒã™ã‚‹å±¥æ­´ã‚µã‚¤ã‚º
            improvement_callback: Callback for improvement suggestions / æ”¹å–„ææ¡ˆç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            locale: Language code for localized messages / ãƒ­ãƒ¼ã‚«ãƒ©ã‚¤ã‚ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨è¨€èªã‚³ãƒ¼ãƒ‰
            next_step: Next step after pipeline execution / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå¾Œã®æ¬¡ã‚¹ãƒ†ãƒƒãƒ—
            store_result_key: Key to store result in context shared_state / ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå…±æœ‰çŠ¶æ…‹ã«çµæœã‚’æ ¼ç´ã™ã‚‹ã‚­ãƒ¼
        """
        # Initialize Step base class
        # StepåŸºåº•ã‚¯ãƒ©ã‚¹ã‚’åˆæœŸåŒ–
        super().__init__(name)
        
        # Store flow-specific configuration
        # ãƒ•ãƒ­ãƒ¼å›ºæœ‰ã®è¨­å®šã‚’ä¿å­˜
        self.next_step = next_step
        self.store_result_key = store_result_key or f"{name}_result"
        
        # Create internal LLMPipeline instance
        # å†…éƒ¨LLMPipelineã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        self.llm_pipeline = LLMPipeline(
            name=f"{name}_pipeline",
            generation_instructions=generation_instructions,
            evaluation_instructions=evaluation_instructions,
            output_model=output_model,
            model=model,
            evaluation_model=evaluation_model,
            temperature=temperature,
            max_tokens=max_tokens,
            timeout=timeout,
            threshold=threshold,
            max_retries=max_retries,
            input_guardrails=input_guardrails,
            output_guardrails=output_guardrails,
            session_history=session_history,
            history_size=history_size,
            improvement_callback=improvement_callback,
            locale=locale,
            tools=tools,
            mcp_servers=mcp_servers,
        )

    async def run(self, user_input: Optional[str], ctx: Context) -> Context:
        """
        Execute GenAgent step using LLMPipeline
        LLMPipelineã‚’ä½¿ç”¨ã—ã¦GenAgentã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹

        Args:
            user_input: User input for the pipeline / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
            ctx: Current workflow context / ç¾åœ¨ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            Context: Updated context with pipeline results / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœä»˜ãæ›´æ–°æ¸ˆã¿ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        # English: Update step information in context
        # æ—¥æœ¬èª: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¹ãƒ†ãƒƒãƒ—æƒ…å ±ã‚’æ›´æ–°
        ctx.update_step_info(self.name)
        
        try:
            # English: Determine input text for pipeline
            # æ—¥æœ¬èª: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ç”¨å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’æ±ºå®š
            input_text = user_input or ctx.last_user_input or ""
            
            if not input_text:
                # English: If no input available, add system message and continue
                # æ—¥æœ¬èª: å…¥åŠ›ãŒãªã„å ´åˆã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ ã—ã¦ç¶šè¡Œ
                ctx.add_system_message(f"GenAgent {self.name}: No input available, skipping pipeline execution")
                result = None
            else:
                # English: Execute LLMPipeline synchronously (no async issues)
                # æ—¥æœ¬èª: LLMPipelineã‚’åŒæœŸçš„ã«å®Ÿè¡Œï¼ˆéåŒæœŸå•é¡Œãªã—ï¼‰
                llm_result = self.llm_pipeline.run(input_text)
                result = llm_result.content if llm_result.success else None
            
            # English: Store result in context
            # æ—¥æœ¬èª: çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä¿å­˜
            if result is not None:
                # English: Store in shared state for other steps to access
                # æ—¥æœ¬èª: ä»–ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†å…±æœ‰çŠ¶æ…‹ã«ä¿å­˜
                ctx.shared_state[self.store_result_key] = result
                ctx.prev_outputs[self.name] = result
                
                # English: Add result as assistant message
                # æ—¥æœ¬èª: çµæœã‚’ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦è¿½åŠ 
                ctx.add_assistant_message(str(result))
                
                # English: Add success system message
                # æ—¥æœ¬èª: æˆåŠŸã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                ctx.add_system_message(f"GenAgent {self.name}: Pipeline executed successfully")
            else:
                # English: Handle case where pipeline returned None (evaluation failed)
                # æ—¥æœ¬èª: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒNoneã‚’è¿”ã—ãŸå ´åˆï¼ˆè©•ä¾¡å¤±æ•—ï¼‰ã‚’å‡¦ç†
                ctx.shared_state[self.store_result_key] = None
                ctx.prev_outputs[self.name] = None
                
                # English: Add failure system message
                # æ—¥æœ¬èª: å¤±æ•—ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
                ctx.add_system_message(f"GenAgent {self.name}: Pipeline execution failed (evaluation threshold not met)")
                
        except Exception as e:
            # English: Handle execution errors
            # æ—¥æœ¬èª: å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã‚’å‡¦ç†
            error_msg = f"GenAgent {self.name} execution error: {str(e)}"
            ctx.add_system_message(error_msg)
            ctx.shared_state[self.store_result_key] = None
            ctx.prev_outputs[self.name] = None
            
            # English: Log error for debugging
            # æ—¥æœ¬èª: ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
            print(f"ğŸš¨ {error_msg}")
        
        # English: Set next step if specified
        # æ—¥æœ¬èª: æŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æ¬¡ã‚¹ãƒ†ãƒƒãƒ—ã‚’è¨­å®š
        if self.next_step:
            ctx.goto(self.next_step)
        
        return ctx

    def get_pipeline_history(self) -> List[Dict[str, Any]]:
        """
        Get the internal pipeline history
        å†…éƒ¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å±¥æ­´ã‚’å–å¾—ã™ã‚‹

        Returns:
            List[Dict[str, Any]]: Pipeline history / ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å±¥æ­´
        """
        return self.llm_pipeline.get_history()

    def get_session_history(self) -> Optional[List[str]]:
        """
        Get the session history
        ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´ã‚’å–å¾—ã™ã‚‹

        Returns:
            Optional[List[str]]: Session history / ã‚»ãƒƒã‚·ãƒ§ãƒ³å±¥æ­´
        """
        return self.llm_pipeline.session_history

    def update_instructions(
        self, 
        generation_instructions: Optional[str] = None,
        evaluation_instructions: Optional[str] = None
    ) -> None:
        """
        Update pipeline instructions
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æŒ‡ç¤ºã‚’æ›´æ–°ã™ã‚‹

        Args:
            generation_instructions: New generation instructions / æ–°ã—ã„ç”ŸæˆæŒ‡ç¤º
            evaluation_instructions: New evaluation instructions / æ–°ã—ã„è©•ä¾¡æŒ‡ç¤º
        """
        self.llm_pipeline.update_instructions(generation_instructions, evaluation_instructions)

    def clear_history(self) -> None:
        """
        Clear pipeline history
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å±¥æ­´ã‚’ã‚¯ãƒªã‚¢
        """
        self.llm_pipeline.clear_history()

    def set_threshold(self, threshold: float) -> None:
        """
        Update evaluation threshold
        è©•ä¾¡é–¾å€¤ã‚’æ›´æ–°ã™ã‚‹

        Args:
            threshold: New threshold value (0-100) / æ–°ã—ã„é–¾å€¤ï¼ˆ0-100ï¼‰
        """
        self.llm_pipeline.set_threshold(threshold)

    def __str__(self) -> str:
        return f"GenAgent({self.name}, model={self.llm_pipeline.model})"

    def __repr__(self) -> str:
        return self.__str__()


# Modern utility functions for creating GenAgent with common configurations
# ãƒ¢ãƒ€ãƒ³ãªGenAgentä½œæˆç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°

def create_simple_gen_agent(
    name: str,
    instructions: str,
    model: str = "gpt-4o-mini",
    next_step: Optional[str] = None,
    threshold: float = 85.0,
    retries: int = 3,
    tools: Optional[List[Dict]] = None,
    mcp_servers: Optional[List[str]] = None
) -> GenAgent:
    """
    Create a simple GenAgent with basic configuration
    åŸºæœ¬è¨­å®šã§ã‚·ãƒ³ãƒ—ãƒ«ãªGenAgentã‚’ä½œæˆ

    Args:
        name: Agent name / ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå
        instructions: Generation instructions / ç”ŸæˆæŒ‡ç¤º
        model: LLM model name / LLMãƒ¢ãƒ‡ãƒ«å
        next_step: Next step name / æ¬¡ã‚¹ãƒ†ãƒƒãƒ—å
        threshold: Evaluation threshold / è©•ä¾¡é–¾å€¤
        retries: Retry attempts / ãƒªãƒˆãƒ©ã‚¤å›æ•°
        tools: OpenAI function tools / OpenAIé–¢æ•°ãƒ„ãƒ¼ãƒ«
        mcp_servers: MCP server identifiers / MCPã‚µãƒ¼ãƒãƒ¼è­˜åˆ¥å­

    Returns:
        GenAgent: Configured agent / è¨­å®šæ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    """
    return GenAgent(
        name=name,
        generation_instructions=instructions,
        model=model,
        next_step=next_step,
        threshold=threshold,
        max_retries=retries,
        tools=tools,
        mcp_servers=mcp_servers
    )


def create_evaluated_gen_agent(
    name: str,
    generation_instructions: str,
    evaluation_instructions: str,
    model: str = "gpt-4o-mini",
    evaluation_model: Optional[str] = None,
    next_step: Optional[str] = None,
    threshold: float = 85.0,
    retries: int = 3,
    tools: Optional[List[Dict]] = None,
    mcp_servers: Optional[List[str]] = None
) -> GenAgent:
    """
    Create a GenAgent with evaluation capabilities
    è©•ä¾¡æ©Ÿèƒ½ä»˜ãGenAgentã‚’ä½œæˆ

    Args:
        name: Agent name / ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå
        generation_instructions: Generation instructions / ç”ŸæˆæŒ‡ç¤º
        evaluation_instructions: Evaluation instructions / è©•ä¾¡æŒ‡ç¤º
        model: LLM model name / LLMãƒ¢ãƒ‡ãƒ«å
        evaluation_model: Evaluation model name / è©•ä¾¡ãƒ¢ãƒ‡ãƒ«å
        next_step: Next step name / æ¬¡ã‚¹ãƒ†ãƒƒãƒ—å
        threshold: Evaluation threshold / è©•ä¾¡é–¾å€¤
        retries: Retry attempts / ãƒªãƒˆãƒ©ã‚¤å›æ•°

    Returns:
        GenAgent: Configured agent with evaluation / è©•ä¾¡æ©Ÿèƒ½ä»˜ãè¨­å®šæ¸ˆã¿ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    """
    return GenAgent(
        name=name,
        generation_instructions=generation_instructions,
        evaluation_instructions=evaluation_instructions,
        model=model,
        evaluation_model=evaluation_model,
        next_step=next_step,
        threshold=threshold,
        max_retries=retries,
        tools=tools,
        mcp_servers=mcp_servers
    )


