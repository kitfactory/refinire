"""
LLMPipeline and GenAgentV2 Example - Modern replacement for deprecated AgentPipeline
LLMPipelineã¨GenAgentV2ã®ä¾‹ - éæ¨å¥¨ã®AgentPipelineã«ä»£ã‚ã‚‹ãƒ¢ãƒ€ãƒ³ãªå®Ÿè£…
"""

import asyncio
from typing import Optional
from pydantic import BaseModel

from agents_sdk_models import (
    LLMPipeline, GenAgentV2, Flow, Context,
    create_simple_llm_pipeline, create_evaluated_llm_pipeline,
    create_simple_gen_agent_v2, create_evaluated_gen_agent_v2,
    create_tool_enabled_llm_pipeline, create_calculator_pipeline,
    create_web_search_pipeline
)


# Example data models for structured output
# æ§‹é€ åŒ–å‡ºåŠ›ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
class TaskAnalysis(BaseModel):
    """Task analysis result / ã‚¿ã‚¹ã‚¯åˆ†æçµæœ"""
    task_type: str
    complexity: str
    estimated_time: str
    requirements: list[str]


class TaskPlan(BaseModel):
    """Task execution plan / ã‚¿ã‚¹ã‚¯å®Ÿè¡Œè¨ˆç”»"""
    steps: list[str]
    resources: list[str]
    timeline: str
    success_criteria: str


def example_basic_llm_pipeline():
    """
    Basic LLMPipeline usage example
    åŸºæœ¬çš„ãªLLMPipelineã®ä½¿ç”¨ä¾‹
    """
    print("ğŸ”§ Basic LLMPipeline Example")
    print("=" * 50)
    
    # Create simple pipeline
    # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = create_simple_llm_pipeline(
        name="task_helper",
        instructions="You are a helpful task planning assistant. Analyze user requests and provide structured guidance.",
        model="gpt-4o-mini"
    )
    
    # Example usage
    # ä½¿ç”¨ä¾‹
    user_input = "I need to organize a team meeting for 10 people next week"
    
    print(f"ğŸ“ User Input: {user_input}")
    print("\nğŸ¤– Processing...")
    
    # Note: This would require actual OpenAI API key to run
    # æ³¨æ„ï¼šå®Ÿéš›ã«å®Ÿè¡Œã™ã‚‹ã«ã¯OpenAI APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™
    try:
        result = pipeline.run(user_input)
        
        if result.success:
            print(f"âœ… Success! Generated response:")
            print(f"ğŸ“„ Content: {result.content}")
            print(f"ğŸ”„ Attempts: {result.attempts}")
        else:
            print(f"âŒ Failed: {result.metadata.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def example_evaluated_llm_pipeline():
    """
    LLMPipeline with evaluation example
    è©•ä¾¡æ©Ÿèƒ½ä»˜ãLLMPipelineã®ä¾‹
    """
    print("ğŸ” Evaluated LLMPipeline Example")
    print("=" * 50)
    
    # Create pipeline with evaluation
    # è©•ä¾¡æ©Ÿèƒ½ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = create_evaluated_llm_pipeline(
        name="quality_writer",
        generation_instructions="""
        You are a professional content writer. Create high-quality, engaging content 
        based on user requests. Focus on clarity, structure, and value.
        """,
        evaluation_instructions="""
        Evaluate the generated content on:
        1. Clarity and readability (0-25 points)
        2. Structure and organization (0-25 points)  
        3. Value and usefulness (0-25 points)
        4. Engagement and style (0-25 points)
        
        Provide a total score out of 100 and brief feedback.
        """,
        model="gpt-4o-mini",
        threshold=80.0,
        max_retries=2
    )
    
    user_input = "Write a brief guide on effective remote work practices"
    
    print(f"ğŸ“ User Input: {user_input}")
    print(f"ğŸ¯ Quality Threshold: {pipeline.threshold}%")
    print("\nğŸ¤– Processing with evaluation...")
    
    try:
        result = pipeline.run(user_input)
        
        if result.success:
            print(f"âœ… Success! High-quality content generated:")
            print(f"ğŸ“„ Content: {result.content[:200]}...")
            print(f"â­ Evaluation Score: {result.evaluation_score}%")
            print(f"ğŸ”„ Attempts: {result.attempts}")
        else:
            print(f"âŒ Failed to meet quality threshold: {result.metadata.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def example_structured_output_pipeline():
    """
    LLMPipeline with structured output example
    æ§‹é€ åŒ–å‡ºåŠ›ä»˜ãLLMPipelineã®ä¾‹
    """
    print("ğŸ“Š Structured Output LLMPipeline Example")
    print("=" * 50)
    
    # Create pipeline with structured output
    # æ§‹é€ åŒ–å‡ºåŠ›ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = LLMPipeline(
        name="task_analyzer",
        generation_instructions="""
        Analyze the given task and provide structured analysis.
        Return your response as JSON with the following structure:
        {
            "task_type": "category of the task",
            "complexity": "low/medium/high",
            "estimated_time": "time estimate",
            "requirements": ["list", "of", "requirements"]
        }
        """,
        output_model=TaskAnalysis,
        model="gpt-4o-mini"
    )
    
    user_input = "Create a mobile app for expense tracking"
    
    print(f"ğŸ“ User Input: {user_input}")
    print("\nğŸ¤– Analyzing task structure...")
    
    try:
        result = pipeline.run(user_input)
        
        if result.success and isinstance(result.content, TaskAnalysis):
            analysis = result.content
            print(f"âœ… Structured Analysis Complete:")
            print(f"ğŸ“‹ Task Type: {analysis.task_type}")
            print(f"âš¡ Complexity: {analysis.complexity}")
            print(f"â±ï¸  Estimated Time: {analysis.estimated_time}")
            print(f"ğŸ“ Requirements:")
            for req in analysis.requirements:
                print(f"   â€¢ {req}")
        else:
            print(f"âŒ Failed to generate structured output")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


async def example_gen_agent_v2_in_flow():
    """
    GenAgentV2 in Flow workflow example
    Flowãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã®GenAgentV2ã®ä¾‹
    """
    print("ğŸ”„ GenAgentV2 in Flow Example")
    print("=" * 50)
    
    # Create GenAgentV2 steps
    # GenAgentV2ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½œæˆ
    analyzer = create_simple_gen_agent_v2(
        name="task_analyzer",
        instructions="""
        Analyze the user's task request and identify key requirements, 
        complexity, and initial planning considerations.
        """,
        next_step="planner"
    )
    
    planner = create_evaluated_gen_agent_v2(
        name="task_planner", 
        generation_instructions="""
        Based on the task analysis, create a detailed execution plan with
        specific steps, required resources, timeline, and success criteria.
        """,
        evaluation_instructions="""
        Evaluate the plan on:
        1. Completeness and detail (0-30 points)
        2. Feasibility and practicality (0-30 points)
        3. Clear timeline and milestones (0-20 points)
        4. Success criteria definition (0-20 points)
        
        Provide total score out of 100.
        """,
        threshold=85.0,
        next_step="reviewer"
    )
    
    reviewer = create_simple_gen_agent_v2(
        name="plan_reviewer",
        instructions="""
        Review the task analysis and execution plan. Provide final 
        recommendations, potential risks, and optimization suggestions.
        """
    )
    
    # Create Flow
    # Flowã‚’ä½œæˆ
    flow = Flow(
        name="task_planning_flow",
        steps=[analyzer, planner, reviewer],
        max_steps=10
    )
    
    print("ğŸ—ï¸  Created task planning workflow with 3 GenAgentV2 steps")
    print("ğŸ“‹ Steps: Analyzer â†’ Planner â†’ Reviewer")
    
    # Example execution (would require API key)
    # å®Ÿè¡Œä¾‹ï¼ˆAPIã‚­ãƒ¼ãŒå¿…è¦ï¼‰
    user_input = "Plan a company retreat for 50 employees"
    
    print(f"\nğŸ“ User Input: {user_input}")
    print("ğŸ¤– Processing through workflow...")
    
    try:
        # Create context and run flow
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆã—ã¦Flowã‚’å®Ÿè¡Œ
        ctx = Context()
        ctx.last_user_input = user_input
        
        # Note: This would require actual OpenAI API key
        # æ³¨æ„ï¼šå®Ÿéš›ã®OpenAI APIã‚­ãƒ¼ãŒå¿…è¦
        # result_ctx = await flow.run(ctx)
        
        print("âœ… Workflow would execute:")
        print("   1. ğŸ” Analyzer: Analyze retreat requirements")
        print("   2. ğŸ“‹ Planner: Create detailed execution plan") 
        print("   3. ğŸ‘€ Reviewer: Review and optimize plan")
        print("\nğŸ’¡ Each step uses LLMPipeline internally (no async issues!)")
        
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def example_pipeline_features():
    """
    Demonstrate advanced LLMPipeline features
    LLMPipelineã®é«˜åº¦ãªæ©Ÿèƒ½ã®ãƒ‡ãƒ¢
    """
    print("âš™ï¸  Advanced LLMPipeline Features")
    print("=" * 50)
    
    # Input guardrails
    # å…¥åŠ›ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«
    def content_filter(text: str) -> bool:
        """Filter inappropriate content / ä¸é©åˆ‡ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿"""
        blocked_words = ["spam", "inappropriate"]
        return not any(word in text.lower() for word in blocked_words)
    
    def length_filter(text: str) -> bool:
        """Filter overly long inputs / é•·ã™ãã‚‹å…¥åŠ›ã‚’ãƒ•ã‚£ãƒ«ã‚¿"""
        return len(text) <= 500
    
    # Output guardrails  
    # å‡ºåŠ›ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«
    def quality_filter(text: str) -> bool:
        """Ensure minimum quality output / æœ€ä½å“è³ªã®å‡ºåŠ›ã‚’ä¿è¨¼"""
        return len(text) > 10 and not text.lower().startswith("i cannot")
    
    # Create pipeline with guardrails
    # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = LLMPipeline(
        name="guarded_assistant",
        generation_instructions="Provide helpful and appropriate responses to user queries.",
        input_guardrails=[content_filter, length_filter],
        output_guardrails=[quality_filter],
        history_size=5,
        max_retries=2,
        model="gpt-4o-mini"
    )
    
    print("ğŸ›¡ï¸  Created pipeline with guardrails:")
    print("   â€¢ Input: Content filter + Length limit")
    print("   â€¢ Output: Quality assurance")
    print("   â€¢ History: Last 5 interactions")
    print("   â€¢ Retries: Up to 2 attempts")
    
    # Test guardrails
    # ã‚¬ãƒ¼ãƒ‰ãƒ¬ãƒ¼ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
    test_inputs = [
        "What is machine learning?",  # Valid
        "This is spam content",       # Blocked by content filter
        "a" * 600                     # Blocked by length filter
    ]
    
    for i, test_input in enumerate(test_inputs, 1):
        print(f"\nğŸ§ª Test {i}: {test_input[:50]}{'...' if len(test_input) > 50 else ''}")
        
        try:
            # Simulate validation (without actual API call)
            # æ¤œè¨¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆï¼ˆå®Ÿéš›ã®APIå‘¼ã³å‡ºã—ãªã—ï¼‰
            input_valid = all(guard(test_input) for guard in pipeline.input_guardrails)
            
            if input_valid:
                print("   âœ… Input passed guardrails")
            else:
                print("   âŒ Input blocked by guardrails")
                
        except Exception as e:
            print(f"   âš ï¸  Error: {e}")
    
    print("\n" + "=" * 50)


def example_tool_enabled_pipeline():
    """
    Tool-enabled LLMPipeline example
    toolæ©Ÿèƒ½ä»˜ãLLMPipelineã®ä¾‹
    """
    print("ğŸ› ï¸  Tool-Enabled LLMPipeline Example")
    print("=" * 50)
    
    # Define custom tools
    # ã‚«ã‚¹ã‚¿ãƒ toolã‚’å®šç¾©
    def get_weather(city: str) -> str:
        """Get the current weather for a city"""
        # Simulated weather data
        weather_data = {
            "Tokyo": "Sunny, 22Â°C",
            "London": "Rainy, 15Â°C", 
            "New York": "Cloudy, 18Â°C",
            "Paris": "Partly cloudy, 20Â°C"
        }
        return weather_data.get(city, f"Weather data not available for {city}")
    
    def calculate_age(birth_year: int) -> int:
        """Calculate age from birth year"""
        from datetime import datetime
        current_year = datetime.now().year
        return current_year - birth_year
    
    def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:
        """Convert currency (simplified rates)"""
        # Simplified exchange rates
        rates = {
            ("USD", "JPY"): 150.0,
            ("USD", "EUR"): 0.85,
            ("EUR", "JPY"): 160.0,
            ("JPY", "USD"): 1/150.0,
            ("EUR", "USD"): 1/0.85,
            ("JPY", "EUR"): 1/160.0
        }
        
        rate = rates.get((from_currency, to_currency), 1.0)
        converted = amount * rate
        return f"{amount} {from_currency} = {converted:.2f} {to_currency}"
    
    # Create pipeline with tools
    # toolä»˜ããƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = create_tool_enabled_llm_pipeline(
        name="multi_tool_assistant",
        instructions="""
        You are a helpful assistant with access to multiple tools:
        - get_weather: Get weather information for cities
        - calculate_age: Calculate age from birth year
        - convert_currency: Convert between currencies
        
        Use these tools when users ask relevant questions.
        """,
        tools=[get_weather, calculate_age, convert_currency],
        model="gpt-4o-mini"
    )
    
    # Test complex query requiring multiple tools
    # è¤‡æ•°toolãŒå¿…è¦ãªè¤‡é›‘ãªã‚¯ã‚¨ãƒªã‚’ãƒ†ã‚¹ãƒˆ
    user_input = "I was born in 1990, what's my age? Also, what's the weather in Tokyo and how much is 100 USD in JPY?"
    
    print(f"ğŸ“ User Input: {user_input}")
    print(f"ğŸ› ï¸  Available Tools: {pipeline.list_tools()}")
    print("\nğŸ¤– Processing with tools...")
    
    try:
        result = pipeline.run(user_input)
        
        if result.success:
            print(f"âœ… Success! AI used tools automatically:")
            print(f"ğŸ“„ Response: {result.content}")
            print(f"ğŸ”„ Attempts: {result.attempts}")
            print(f"ğŸ“Š Metadata: {result.metadata}")
        else:
            print(f"âŒ Failed: {result.metadata.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def example_calculator_pipeline():
    """
    Calculator pipeline example  
    è¨ˆç®—æ©Ÿãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¾‹
    """
    print("ğŸ§® Calculator LLMPipeline Example")
    print("=" * 50)
    
    # Create calculator pipeline
    # è¨ˆç®—æ©Ÿãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = create_calculator_pipeline(
        name="math_assistant",
        model="gpt-4o-mini"
    )
    
    user_input = "Calculate the area of a circle with radius 5, and then find the square root of that result"
    
    print(f"ğŸ“ User Input: {user_input}")
    print(f"ğŸ› ï¸  Available Tools: {pipeline.list_tools()}")
    print("\nğŸ¤– Processing mathematical query...")
    
    try:
        result = pipeline.run(user_input)
        
        if result.success:
            print(f"âœ… Success! Mathematical calculation completed:")
            print(f"ğŸ“„ Response: {result.content}")
        else:
            print(f"âŒ Failed: {result.metadata.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def example_web_search_pipeline():
    """
    Web search pipeline example
    Webæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ä¾‹
    """
    print("ğŸ” Web Search LLMPipeline Example")
    print("=" * 50)
    
    # Create web search pipeline
    # Webæ¤œç´¢ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = create_web_search_pipeline(
        name="search_assistant",
        model="gpt-4o-mini"
    )
    
    user_input = "What are the latest developments in AI technology?"
    
    print(f"ğŸ“ User Input: {user_input}")
    print(f"ğŸ› ï¸  Available Tools: {pipeline.list_tools()}")
    print("\nğŸ¤– Processing search query...")
    
    try:
        result = pipeline.run(user_input)
        
        if result.success:
            print(f"âœ… Success! Search completed:")
            print(f"ğŸ“„ Response: {result.content}")
            print("ğŸ’¡ Note: This uses a placeholder search implementation")
        else:
            print(f"âŒ Failed: {result.metadata.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def example_manual_tool_management():
    """
    Manual tool management example
    æ‰‹å‹•toolç®¡ç†ã®ä¾‹
    """
    print("âš™ï¸  Manual Tool Management Example")
    print("=" * 50)
    
    # Create basic pipeline
    # åŸºæœ¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
    pipeline = LLMPipeline(
        name="custom_assistant",
        generation_instructions="You are a helpful assistant with access to tools.",
        model="gpt-4o-mini",
        tools=[]  # Start with no tools
    )
    
    # Define and add tools manually
    # toolã‚’æ‰‹å‹•ã§å®šç¾©ãƒ»è¿½åŠ 
    def greet_user(name: str) -> str:
        """Greet a user by name"""
        return f"Hello, {name}! Nice to meet you!"
    
    def get_time() -> str:
        """Get the current time"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Add tools one by one
    # toolã‚’ä¸€ã¤ãšã¤è¿½åŠ 
    pipeline.add_function_tool(greet_user)
    pipeline.add_function_tool(get_time)
    
    print(f"ğŸ› ï¸  Added Tools: {pipeline.list_tools()}")
    
    user_input = "Greet me as Alice and tell me the current time"
    
    print(f"ğŸ“ User Input: {user_input}")
    print("\nğŸ¤– Processing with manually added tools...")
    
    try:
        result = pipeline.run(user_input)
        
        if result.success:
            print(f"âœ… Success! Tools executed:")
            print(f"ğŸ“„ Response: {result.content}")
        else:
            print(f"âŒ Failed: {result.metadata.get('error', 'Unknown error')}")
    
        # Demonstrate tool removal
        # toolå‰Šé™¤ã‚’ãƒ‡ãƒ¢
        print(f"\nğŸ—‘ï¸  Removing 'greet_user' tool...")
        removed = pipeline.remove_tool("greet_user")
        print(f"   Removed: {removed}")
        print(f"ğŸ› ï¸  Remaining Tools: {pipeline.list_tools()}")
            
    except Exception as e:
        print(f"âš ï¸  Note: This example requires OpenAI API key. Error: {e}")
    
    print("\n" + "=" * 50)


def main():
    """
    Run all examples
    å…¨ã¦ã®ä¾‹ã‚’å®Ÿè¡Œ
    """
    print("ğŸš€ LLMPipeline & GenAgentV2 Examples")
    print("Modern replacement for deprecated AgentPipeline")
    print("éæ¨å¥¨ã®AgentPipelineã«ä»£ã‚ã‚‹ãƒ¢ãƒ€ãƒ³ãªå®Ÿè£…\n")
    
    # Basic examples
    # åŸºæœ¬ä¾‹
    example_basic_llm_pipeline()
    example_evaluated_llm_pipeline()
    example_structured_output_pipeline()
    
    # Advanced features
    # é«˜åº¦ãªæ©Ÿèƒ½
    example_pipeline_features()
    
    # Flow integration
    # Flowçµ±åˆ
    print("ğŸ”„ Running async Flow example...")
    asyncio.run(example_gen_agent_v2_in_flow())
    
    # New examples
    # æ–°ã—ã„ä¾‹
    example_tool_enabled_pipeline()
    example_calculator_pipeline()
    example_web_search_pipeline()
    example_manual_tool_management()
    
    print("\nğŸ‰ All examples completed!")
    print("\nğŸ’¡ Key Benefits of New Implementation:")
    print("   âœ… No dependency on deprecated AgentPipeline")
    print("   âœ… No async event loop conflicts")
    print("   âœ… Direct OpenAI Python SDK usage")
    print("   âœ… Full Flow/Step architecture support")
    print("   âœ… Comprehensive testing coverage")
    print("   âœ… Future-proof design")


if __name__ == "__main__":
    main() 